{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Leia V3 Retriever Router\n",
    "\n",
    "This version of Leia is a test case for a Router based Query Engine. Instead of giving the tools explicitly, each \"tool\" is a separate index and the router transfers the query to the most suitable one.\n",
    "using a retrieval router means that we can have a very large number of tools and indexes without spamming the context."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import sys"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# NOTE: This is ONLY necessary in jupyter notebook.\n",
    "# Details: Jupyter runs an event-loop behind the scenes.\n",
    "#          This results in nested event-loops when we start an event-loop to make async queries.\n",
    "#          This is normally not allowed, we use nest_asyncio to allow it for convenience.\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Read and set API keys"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Open and read the config file\n",
    "with open('config.json', 'r') as config_file:\n",
    "    config_data = json.load(config_file)\n",
    "\n",
    "# Retrieve the API key from the config data\n",
    "api_key = config_data['api_key']\n",
    "os.environ['OPENAI_API_KEY'] = api_key"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Logging"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "import logging\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO) #DEBUG, INFO, WARNING, ERROR, CRITICAL\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:Note: NumExpr detected 10 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "Note: NumExpr detected 10 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n",
      "NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "from llama_index.callbacks import CallbackManager, TokenCountingHandler\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "\n",
    "# you can set a tokenizer directly, or optionally let it default\n",
    "# to the same tokenizer that was used previously for token counting\n",
    "# NOTE: The tokenizer should be a function that takes in text and returns a list of tokens\n",
    "token_counter = TokenCountingHandler(\n",
    "    tokenizer=tiktoken.encoding_for_model(\"gpt-4\").encode,\n",
    "    verbose=True  # set to true to see usage printed to the console\n",
    "    )\n",
    "callback_manager = CallbackManager([token_counter])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set up query engine for each index separately"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load the Actions index"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.loading:Loading all indices.\n",
      "Loading all indices.\n"
     ]
    }
   ],
   "source": [
    "from llama_index import StorageContext, load_index_from_storage\n",
    "\n",
    "# rebuild storage context\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"./storage/actions\")\n",
    "# load index\n",
    "actions_index = load_index_from_storage(storage_context)\n",
    "actions_engine = actions_index.as_query_engine(similarity_top_k=3) #uses the default llm! Not gpt4!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load the Controls index"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load the Documentation index"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.loading:Loading all indices.\n",
      "Loading all indices.\n"
     ]
    }
   ],
   "source": [
    "# rebuild storage context\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"./storage/documentation\")\n",
    "# load index\n",
    "docu_index = load_index_from_storage(storage_context)\n",
    "docu_engine = docu_index.as_query_engine(similarity_top_k=3) #uses the default llm! Not gpt4!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## setup tool retriever query engine"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from llama_index.tools import QueryEngineTool, ToolMetadata\n",
    "\n",
    "query_engine_tools = [\n",
    "    QueryEngineTool(\n",
    "        query_engine=actions_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"LeiaActions\",\n",
    "            description=\"provides documentation and descriptions for all implemented LeiaActions. try finding the appropriate action for a given user request and fill in the arguments. in case a required argument is not provided by context or user input, ask the user for it. LeiaActions are intended to be used under the hood of LiquidEarth, so the user should not be aware of them.to trigger an action, append the output of the tool to your answer in the following form [TRIGGERACTION nameOfAction(arguments)]. Never return any actions that you are not sure about!\"\n",
    "        )\n",
    "    ),\n",
    "    QueryEngineTool(\n",
    "        query_engine=docu_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"Documentation\",\n",
    "            description=\"provides general information, user manual and the roadmap for LiquidEarth. pass the entire user request to the tool to find the appropriate documentation and use it for your response.\"\n",
    "        )\n",
    "    ),\n",
    "\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from llama_index.objects import ObjectIndex, SimpleToolNodeMapping\n",
    "\n",
    "tool_mapping = SimpleToolNodeMapping.from_objects(query_engine_tools)\n",
    "obj_index = ObjectIndex.from_objects(\n",
    "    query_engine_tools,\n",
    "    tool_mapping,\n",
    "    VectorStoreIndex,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## custom Agent (wip)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from llama_index.llms import OpenAI\n",
    "from llama_index import ServiceContext\n",
    "\n",
    "gpt4 = OpenAI(temperature=0, model=\"gpt-4\")\n",
    "service_context_gpt4 = ServiceContext.from_defaults(llm=gpt4,callback_manager=callback_manager)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### create Prompt Template"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from llama_index import Prompt\n",
    "# define custom Prompt\n",
    "TEMPLATE_STR = (\n",
    "    \"You are Leia, the LiquidEarth Intelligent Assistant. You are helping a user with a question about LiquidEarth. You are very smart and friendly and always in a great mood.\\n\"\n",
    "    \"In LiquidEarth, a 'Space' and a 'Project are the same thing. We have provided Documentation on the software and further information below. In some cases the metadata includes a 'Control' Field that points to a UI element in the app associated to the described functionality. this is only for internal use. when describing controls to the user, use descriptions and names from the text, not the 'control' values. \\n\"\n",
    "    \"If the user asks you to do something in LiquidEarth, look for the right 'LeiaAction' in the context and compile the function call based on the information and example. do not use any Actions that are not documented. return the function call at the end of your response in curly braces. \\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"Answer the question for a human to understand. Additionally, return the 'Control' properties in the order of operations in the following form at the end of your response: [[control1], [control2], ...]. Append the list to your response without further comment. If no controls are found, do not comment it. Never include any controls that are not specified in the Metadata Field in the provided documentation. Do not interpret any controls from the text body. If the answer requires multiple steps, describe each step in detail. Given this information, please answer the question: {query_str}\\n\"\n",
    ")\n",
    "QA_TEMPLATE = Prompt(TEMPLATE_STR)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "from llama_index.query_engine import ToolRetrieverRouterQueryEngine\n",
    "\n",
    "\n",
    "query_engine = ToolRetrieverRouterQueryEngine(obj_index.as_retriever(),service_context_gpt4)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#dirty hack: trying to increase the context size\n",
    "query_engine.retriever._similarity_top_k = 10"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.query_engine.router_query_engine:Combining responses from multiple query engines.\n",
      "Combining responses from multiple query engines.\n",
      "LLM Prompt Token Usage: 112\n",
      "LLM Completion Token Usage: 42\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"what can you do for me?\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the context information, I can provide you with answers to frequently asked questions about the basics and guide you on how to change the background color of the LiquidEarth Workspace using the LeiaActionsSwitchBackgroundColor command.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Chat with a prompt template (ToDo)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "custom_prompt = Prompt(\"\"\"\\\n",
    "Given a conversation (between Human and Assistant) and a follow up message from Human, \\\n",
    "rewrite the message to be a standalone question that captures all relevant context \\\n",
    "from the conversation.\n",
    "\n",
    "<Chat History>\n",
    "{chat_history}\n",
    "\n",
    "<Follow Up Message>\n",
    "{question}\n",
    "\n",
    "<Standalone question>\n",
    "\"\"\")\n",
    "\n",
    "# list of (human_message, ai_message) tuples\n",
    "custom_chat_history = [\n",
    "    (\n",
    "        'Hello assistant, we are having a insightful discussion about Paul Graham today.',\n",
    "        'Okay, sounds good.'\n",
    "    )\n",
    "]\n",
    "\n",
    "query_engine = index.as_query_engine()\n",
    "chat_engine = CondenseQuestionChatEngine.from_defaults(\n",
    "    query_engine=query_engine,\n",
    "    condense_question_prompt=custom_prompt,\n",
    "    chat_history=custom_chat_history,\n",
    "    verbose=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## print token usage"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Embedding Tokens: ', token_counter.total_embedding_token_count, '\\n',\n",
    "      'LLM Prompt Tokens: ', token_counter.prompt_llm_token_count, '\\n',\n",
    "      'LLM Completion Tokens: ', token_counter.completion_llm_token_count, '\\n',\n",
    "      'Total LLM Token Count: ', token_counter.total_llm_token_count)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
