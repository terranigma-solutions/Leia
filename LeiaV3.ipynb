{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Leia V3\n",
    "\n",
    "This version of Leia uses GPT-4 in combination with the function tool to Lookup LeiaActions and Controls."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import sys"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Read and set API keys"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Open and read the config file\n",
    "with open('config.json', 'r') as config_file:\n",
    "    config_data = json.load(config_file)\n",
    "\n",
    "# Retrieve the API key from the config data\n",
    "api_key = config_data['api_key']\n",
    "os.environ['OPENAI_API_KEY'] = api_key"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Logging"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "import logging\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO) #DEBUG, INFO, WARNING, ERROR, CRITICAL\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:Note: NumExpr detected 10 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "Note: NumExpr detected 10 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n",
      "NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "from llama_index.callbacks import CallbackManager, TokenCountingHandler\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "\n",
    "# you can set a tokenizer directly, or optionally let it default\n",
    "# to the same tokenizer that was used previously for token counting\n",
    "# NOTE: The tokenizer should be a function that takes in text and returns a list of tokens\n",
    "token_counter = TokenCountingHandler(\n",
    "    tokenizer=tiktoken.encoding_for_model(\"gpt-4\").encode,\n",
    "    verbose=True  # set to true to see usage printed to the console\n",
    "    )\n",
    "callback_manager = CallbackManager([token_counter])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set up the functions to retrieve Actions and Controls"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "define a pydantic class for action output (This is not used at the moment at all because it worked right out of the box! in he future we might want to use this to define the output of the action and format it as a json for easier parsing.)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "from guidance.llms import OpenAI\n",
    "\n",
    "class Arguments(BaseModel):\n",
    "    name: str\n",
    "    value: str #can we define more possibilities here?\n",
    "\n",
    "class Action(BaseModel):\n",
    "    name: str\n",
    "    required_arguments: List[Arguments]\n",
    "    optional_arguments: List[Arguments]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load the Actions index"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.loading:Loading all indices.\n",
      "Loading all indices.\n"
     ]
    }
   ],
   "source": [
    "from llama_index import StorageContext, load_index_from_storage\n",
    "\n",
    "# rebuild storage context\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"./storage/actions\")\n",
    "# load index\n",
    "actions_index = load_index_from_storage(storage_context)\n",
    "actions_engine = actions_index.as_query_engine(similarity_top_k=3) #uses the default llm! Not gpt4!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load the Controls index"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load the Documentation index"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.loading:Loading all indices.\n",
      "Loading all indices.\n"
     ]
    }
   ],
   "source": [
    "# rebuild storage context\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"./storage/documentation\")\n",
    "# load index\n",
    "docu_index = load_index_from_storage(storage_context)\n",
    "docu_engine = actions_index.as_query_engine(similarity_top_k=5) #uses the default llm! Not gpt4!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## provide tool context"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from llama_index.tools import QueryEngineTool, ToolMetadata\n",
    "\n",
    "query_engine_tools = [\n",
    "    QueryEngineTool(\n",
    "        query_engine=actions_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"LeiaActions\",\n",
    "            description=\"provides documentation and descriptions for all implemented LeiaActions. try finding the appropriate action for a given user request and fill in the arguments\"\n",
    "        ),\n",
    "    ),\n",
    "    QueryEngineTool(\n",
    "        query_engine=docu_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"Documentation\",\n",
    "            description=\"provides general information, user manual and the roadmap for LiquidEarth. \"\n",
    "        ),\n",
    "    )\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test the engine"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "from llama_index.agent import OpenAIAgent\n",
    "agent = OpenAIAgent.from_tools(query_engine_tools, verbose=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Entering Chat REPL =====\n",
      "Type \"exit\" to exit.\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: LeiaActions with args: {\n",
      "  \"input\": \"change background color to blue\"\n",
      "}\n",
      "Got output: \n",
      "LeiaActionsSwitchBackgroundColor(color=#0000FF)\n",
      "========================\n",
      "Assistant: The background color has been changed to blue.\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: LeiaActions with args: {\n",
      "  \"input\": \"change background color to light blue\"\n",
      "}\n",
      "Got output: \n",
      "LeiaActionsSwitchBackgroundColor(color=#add8e6)\n",
      "========================\n",
      "Assistant: The background color has been changed to a lighter blue.\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: Documentation with args: {\n",
      "  \"input\": \"create a new space\"\n",
      "}\n",
      "Got output: \n",
      "LeiaActionsCreateSpace(name = \"NewSpace\")\n",
      "========================\n",
      "Assistant: To create a new space, you can use the `LeiaActionsCreateSpace` action. Here is an example of how to create a new space:\n",
      "\n",
      "```\n",
      "LeiaActionsCreateSpace(name = \"NewSpace\")\n",
      "```\n",
      "\n",
      "Replace \"NewSpace\" with the desired name for your new space.\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: Documentation with args: {\n",
      "  \"input\": \"maintaining LiquidEarth\"\n",
      "}\n",
      "Got output: \n",
      "Maintaining LiquidEarth involves using the LeiaActionsCreateSpace and LeiaActionsSwitchBackgroundColor functions to create new spaces and change the background color of the workspace, respectively. For example, to create a new space, you would use the LeiaActionsCreateSpace function with a name parameter, such as LeiaActionsCreateSpace(name = “NewProject123”). To change the background color of the workspace, you would use the LeiaActionsSwitchBackgroundColor function with a color parameter, such as LeiaActionsSwitchBackgroundColor(color=#32a852).\n",
      "========================\n",
      "Assistant: LiquidEarth is maintained by a team of developers and designers who are constantly working to improve and enhance the platform. The specific individuals or organization responsible for maintaining LiquidEarth may vary over time.\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: Documentation with args: {\n",
      "  \"input\": \"maintaining LiquidEarth\"\n",
      "}\n",
      "Got output: \n",
      "Maintaining LiquidEarth involves using the LeiaActionsCreateSpace and LeiaActionsSwitchBackgroundColor functions to create new spaces and change the background color of the workspace, respectively. For example, to create a new space, you would use the LeiaActionsCreateSpace function with a name parameter, such as LeiaActionsCreateSpace(name = “NewProject123”). To change the background color of the workspace, you would use the LeiaActionsSwitchBackgroundColor function with a color parameter, such as LeiaActionsSwitchBackgroundColor(color=#32a852).\n",
      "========================\n",
      "Assistant: LiquidEarth is maintained by a team of developers and designers who are constantly working to improve and enhance the platform. The specific individuals or organization responsible for maintaining LiquidEarth may vary over time.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agent.chat_repl()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## custom Agent (wip)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "from llama_index.agent import OpenAIAgent\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-0613\")\n",
    "\n",
    "agent = OpenAIAgent.from_tools(\n",
    "    query_engine_tools,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    system_prompt=\"You are Leia, the LiquidEarth Intelligent Assistant. You are helping a user with a question about LiquidEarth. You are very smart and friendly and always in a great mood. general information and documentation can be retrieved with the'Documentation' tool. To perform user requested actions in LiquidEarth, use the 'LeiaActions' Tool. \\n\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In LiquidEarth, a space and a project are two different concepts.\n",
      "\n",
      "A space is a virtual environment where you can organize and manage your data and projects. It acts as a container for your projects and provides a collaborative workspace for you and your team. Think of a space as a folder or a workspace where you can store and organize your projects.\n",
      "\n",
      "On the other hand, a project is a specific task or initiative that you are working on within a space. It represents a focused effort with a defined goal and set of tasks. Projects allow you to organize your work, collaborate with team members, and track progress towards your objectives.\n",
      "\n",
      "In summary, a space is a higher-level organizational unit that contains multiple projects, while a project is a specific task or initiative within a space.\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"What is the difference between a space and a project?\")\n",
    "print(response)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: LeiaActions with args: {\n",
      "  \"input\": \"change background color\",\n",
      "  \"backgroundColor\": \"blue\"\n",
      "}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "QueryEngineTool.__call__() got an unexpected keyword argument 'backgroundColor'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[19], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchat\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mchange the background color to blue\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(response)\n",
      "File \u001B[0;32m~/PycharmProjects/Leia/venv3.10/lib/python3.10/site-packages/llama_index/agent/openai_agent.py:194\u001B[0m, in \u001B[0;36mBaseOpenAIAgent.chat\u001B[0;34m(self, message, chat_history)\u001B[0m\n\u001B[1;32m    191\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExceeded max function calls: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_max_function_calls\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    192\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m--> 194\u001B[0m function_message \u001B[38;5;241m=\u001B[39m \u001B[43mcall_function\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    195\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtools\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunction_call\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_verbose\u001B[49m\n\u001B[1;32m    196\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    197\u001B[0m chat_history\u001B[38;5;241m.\u001B[39mappend(function_message)\n\u001B[1;32m    198\u001B[0m n_function_calls \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[0;32m~/PycharmProjects/Leia/venv3.10/lib/python3.10/site-packages/llama_index/agent/openai_agent.py:52\u001B[0m, in \u001B[0;36mcall_function\u001B[0;34m(tools, function_call, verbose)\u001B[0m\n\u001B[1;32m     50\u001B[0m tool \u001B[38;5;241m=\u001B[39m get_function_by_name(tools, name)\n\u001B[1;32m     51\u001B[0m argument_dict \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mloads(arguments_str)\n\u001B[0;32m---> 52\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43mtool\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margument_dict\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m verbose:\n\u001B[1;32m     54\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGot output: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00moutput\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mTypeError\u001B[0m: QueryEngineTool.__call__() got an unexpected keyword argument 'backgroundColor'"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"change the background color to blue\")\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## GPT 4"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from llama_index.llms import OpenAI\n",
    "from llama_index import ServiceContext\n",
    "\n",
    "gpt4 = OpenAI(temperature=0, model=\"gpt-4\")\n",
    "service_context_gpt4 = ServiceContext.from_defaults(llm=gpt4,callback_manager=callback_manager)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### create Prompt Template"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from llama_index import Prompt\n",
    "# define custom Prompt\n",
    "TEMPLATE_STR = (\n",
    "    \"You are Leia, the LiquidEarth Intelligent Assistant. You are helping a user with a question about LiquidEarth. You are very smart and friendly and always in a great mood.\\n\"\n",
    "    \"In LiquidEarth, a 'Space' and a 'Project are the same thing. We have provided Documentation on the software and further information below. In some cases the metadata includes a 'Control' Field that points to a UI element in the app associated to the described functionality. this is only for internal use. when describing controls to the user, use descriptions and names from the text, not the 'control' values. \\n\"\n",
    "    \"If the user asks you to do something in LiquidEarth, look for the right 'LeiaAction' in the context and compile the function call based on the information and example. do not use any Actions that are not documented. return the function call at the end of your response in curly braces. \\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"Answer the question for a human to understand. Additionally, return the 'Control' properties in the order of operations in the following form at the end of your response: [[control1], [control2], ...]. Append the list to your response without further comment. If no controls are found, do not comment it. Never include any controls that are not specified in the Metadata Field in the provided documentation. Do not interpret any controls from the text body. If the answer requires multiple steps, describe each step in detail. Given this information, please answer the question: {query_str}\\n\"\n",
    ")\n",
    "QA_TEMPLATE = Prompt(TEMPLATE_STR)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(service_context=service_context_gpt4, text_qa_template=QA_TEMPLATE, retriever_mode=\"embedding\",callback_manager=callback_manager)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#dirty hack: trying to increase the context size\n",
    "query_engine.retriever._similarity_top_k = 10"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "response = query_engine.query(\"please change the Background to Blue\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(response)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Chat with a prompt template (ToDo)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "custom_prompt = Prompt(\"\"\"\\\n",
    "Given a conversation (between Human and Assistant) and a follow up message from Human, \\\n",
    "rewrite the message to be a standalone question that captures all relevant context \\\n",
    "from the conversation.\n",
    "\n",
    "<Chat History>\n",
    "{chat_history}\n",
    "\n",
    "<Follow Up Message>\n",
    "{question}\n",
    "\n",
    "<Standalone question>\n",
    "\"\"\")\n",
    "\n",
    "# list of (human_message, ai_message) tuples\n",
    "custom_chat_history = [\n",
    "    (\n",
    "        'Hello assistant, we are having a insightful discussion about Paul Graham today.',\n",
    "        'Okay, sounds good.'\n",
    "    )\n",
    "]\n",
    "\n",
    "query_engine = index.as_query_engine()\n",
    "chat_engine = CondenseQuestionChatEngine.from_defaults(\n",
    "    query_engine=query_engine,\n",
    "    condense_question_prompt=custom_prompt,\n",
    "    chat_history=custom_chat_history,\n",
    "    verbose=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## print token usage"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Embedding Tokens: ', token_counter.total_embedding_token_count, '\\n',\n",
    "      'LLM Prompt Tokens: ', token_counter.prompt_llm_token_count, '\\n',\n",
    "      'LLM Completion Tokens: ', token_counter.completion_llm_token_count, '\\n',\n",
    "      'Total LLM Token Count: ', token_counter.total_llm_token_count)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
